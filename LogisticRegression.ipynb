{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "import os\n",
    "import urllib\n",
    "import sys\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.ml.feature import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark session\n",
    "spark = pyspark.sql.SparkSession.builder.appName('Otto').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************\n",
      "Python version: 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 10:22:32) [MSC v.1900 64 bit (AMD64)]\n",
      "Spark version: 2.3.0\n",
      "****************\n"
     ]
    }
   ],
   "source": [
    "# print runtime versions\n",
    "print ('****************')\n",
    "print ('Python version: {}'.format(sys.version))\n",
    "print ('Spark version: {}'.format(spark.version))\n",
    "print ('****************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train.csv into Spark dataframe\n",
    "data = spark.createDataFrame(pd.read_csv('train.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(feat_1=1, feat_2=0, feat_3=0, feat_4=0, feat_5=0, feat_6=0, feat_7=0, feat_8=0, feat_9=0, feat_10=0, feat_11=1, feat_12=0, feat_13=0, feat_14=0, feat_15=0, feat_16=0, feat_17=2, feat_18=0, feat_19=0, feat_20=0, feat_21=0, feat_22=1, feat_23=0, feat_24=4, feat_25=1, feat_26=1, feat_27=0, feat_28=0, feat_29=2, feat_30=0, feat_31=0, feat_32=0, feat_33=0, feat_34=0, feat_35=1, feat_36=0, feat_37=0, feat_38=0, feat_39=0, feat_40=1, feat_41=0, feat_42=5, feat_43=0, feat_44=0, feat_45=0, feat_46=0, feat_47=0, feat_48=2, feat_49=0, feat_50=0, feat_51=0, feat_52=0, feat_53=0, feat_54=1, feat_55=0, feat_56=0, feat_57=2, feat_58=0, feat_59=0, feat_60=11, feat_61=0, feat_62=1, feat_63=1, feat_64=0, feat_65=1, feat_66=0, feat_67=7, feat_68=0, feat_69=0, feat_70=0, feat_71=1, feat_72=0, feat_73=0, feat_74=0, feat_75=0, feat_76=0, feat_77=0, feat_78=0, feat_79=2, feat_80=1, feat_81=0, feat_82=0, feat_83=0, feat_84=0, feat_85=1, feat_86=0, feat_87=0, feat_88=0, feat_89=0, feat_90=0, feat_91=0, feat_92=0, feat_93=0, target='Class_1', features=SparseVector(93, {0: 1.0, 10: 1.0, 16: 2.0, 21: 1.0, 23: 4.0, 24: 1.0, 25: 1.0, 28: 2.0, 34: 1.0, 39: 1.0, 41: 5.0, 47: 2.0, 53: 1.0, 56: 2.0, 59: 11.0, 61: 1.0, 62: 1.0, 64: 1.0, 66: 7.0, 70: 1.0, 78: 2.0, 79: 1.0, 84: 1.0}))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorize all numerical columns into a single feature column\n",
    "feature_cols = data.columns[:-1]\n",
    "assembler = pyspark.ml.feature.VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "data = assembler.transform(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert text labels into indices\n",
    "data = data.select(['features', 'target'])\n",
    "label_indexer = pyspark.ml.feature.StringIndexer(inputCol='target', outputCol='label').fit(data)\n",
    "data = label_indexer.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading for machine learning\n",
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(93,[0,10,16,21,2...|  8.0|\n",
      "|(93,[7,17,36,57,6...|  8.0|\n",
      "|(93,[7,16,32,47,5...|  8.0|\n",
      "|(93,[0,3,4,5,6,9,...|  8.0|\n",
      "|(93,[16,23,40,49,...|  8.0|\n",
      "|(93,[0,1,4,12,16,...|  8.0|\n",
      "|(93,[0,7,9,21,24,...|  8.0|\n",
      "|(93,[16,24,41,48,...|  8.0|\n",
      "|(93,[7,11,12,16,1...|  8.0|\n",
      "|(93,[6,16,19,23,2...|  8.0|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# only select the features and label column\n",
    "data = data.select(['features', 'label'])\n",
    "print(\"Reading for machine learning\")\n",
    "data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change regularization rate and you will likely get a different accuracy.\n",
    "reg = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Logistic Regression to train on the training set\n",
    "train, test = data.randomSplit([0.90, 0.10])\n",
    "lr = pyspark.ml.classification.LogisticRegression(regParam=reg)\n",
    "model = lr.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(93,[0,1,2,3,4,5,...|  8.0|\n",
      "|(93,[0,1,2,3,4,6,...|  8.0|\n",
      "|(93,[0,1,2,4,5,6,...|  8.0|\n",
      "|(93,[0,1,2,6,8,16...|  0.0|\n",
      "|(93,[0,1,2,7,10,1...|  0.0|\n",
      "|(93,[0,1,2,9,10,1...|  0.0|\n",
      "|(93,[0,1,6,7,9,12...|  8.0|\n",
      "|(93,[0,1,6,12,17,...|  8.0|\n",
      "|(93,[0,1,12,13,23...|  0.0|\n",
      "|(93,[0,2,3,4,6,9,...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = spark.createDataFrame(pd.read_csv('test.csv'))\n",
    "feature_cols1 = testdata.columns[:]\n",
    "assembler2 = pyspark.ml.feature.VectorAssembler(inputCols=feature_cols1, outputCol='features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = assembler2.transform(testdata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(93,[9,13,14,15,2...|\n",
      "|(93,[0,1,2,3,14,1...|\n",
      "|(93,[1,2,3,10,11,...|\n",
      "|(93,[3,14,15,24,2...|\n",
      "|(93,[0,3,6,7,9,23...|\n",
      "|(93,[8,23,24,31,4...|\n",
      "|(93,[9,13,15,17,1...|\n",
      "|(93,[0,13,17,23,2...|\n",
      "|(93,[4,13,23,24,3...|\n",
      "|(93,[8,16,23,63,6...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testdata = testdata.select(['features'])\n",
    "testdata.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "|(93,[9,13,14,15,2...|[4.23546802111468...|[0.26974766452314...|       3.0|\n",
      "|(93,[0,1,2,3,14,1...|[0.44813591377910...|[0.01537548634700...|       1.0|\n",
      "|(93,[1,2,3,10,11,...|[-1.2371349239515...|[1.31694338800390...|       1.0|\n",
      "|(93,[3,14,15,24,2...|[5.00641281822240...|[0.69927517997399...|       0.0|\n",
      "|(93,[0,3,6,7,9,23...|[-0.7249191292608...|[0.01665906578707...|       2.0|\n",
      "|(93,[8,23,24,31,4...|[4.42472615054001...|[0.66174542620531...|       0.0|\n",
      "|(93,[9,13,15,17,1...|[-0.4371875302354...|[0.00737054092038...|       2.0|\n",
      "|(93,[0,13,17,23,2...|[3.33615392328478...|[0.71528484899770...|       0.0|\n",
      "|(93,[4,13,23,24,3...|[2.58940292779638...|[0.51675998185098...|       0.0|\n",
      "|(93,[8,16,23,63,6...|[1.41047028988930...|[0.24809920936525...|       2.0|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predict on the test set\n",
    "prediction = model.transform(testdata)\n",
    "print(\"Prediction\")\n",
    "prediction.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = prediction.toPandas().to_csv(\"output.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
